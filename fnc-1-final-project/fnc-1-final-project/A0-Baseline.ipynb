{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Attempt000",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm",
   "background_execution": "on"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Attempt 000\n",
    "\n",
    "Default approach\n",
    "Added Precison F1 score for side-by-side comparison"
   ],
   "metadata": {
    "id": "4wL8vY01xan-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xxp7nOGrx_Se",
    "outputId": "c99ae724-8651-4161-f466-0ca0dd17b251"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from csv import DictReader\n",
    "\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self, name=\"train\", path=\"fnc-1\", is_unlabeled=False):\n",
    "        self.path = path\n",
    "\n",
    "        print(\"Reading dataset\")\n",
    "        bodies = name + \"_bodies.csv\"\n",
    "        stances = name + \"_stances\"\n",
    "        if is_unlabeled is True:\n",
    "            stances = stances + \"_unlabeled\"\n",
    "        stances = stances + \".csv\"\n",
    "\n",
    "        self.stances = self.read(stances)\n",
    "        articles = self.read(bodies)\n",
    "        self.articles = dict()\n",
    "\n",
    "        # make the body ID an integer value\n",
    "        for s in self.stances:\n",
    "            s['Body ID'] = int(s['Body ID'])\n",
    "\n",
    "        # copy all bodies into a dictionary\n",
    "        for article in articles:\n",
    "            self.articles[int(article['Body ID'])] = article['articleBody']\n",
    "\n",
    "        print(\"Total stances: \" + str(len(self.stances)))\n",
    "        print(\"Total bodies: \" + str(len(self.articles)))\n",
    "\n",
    "    def read(self, filename):\n",
    "        rows = []\n",
    "        with open(self.path + \"/\" + filename, \"r\", encoding='utf-8') as table:\n",
    "            r = DictReader(table)\n",
    "\n",
    "            for line in r:\n",
    "                rows.append(line)\n",
    "        return rows\n"
   ],
   "metadata": {
    "id": "pHh0As2dxn6d"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def generate_hold_out_split (dataset, training = 0.8, base_dir=\"splits\"):\n",
    "    r = random.Random()\n",
    "    r.seed(1489215)\n",
    "\n",
    "    article_ids = list(dataset.articles.keys())  # get a list of article ids\n",
    "    r.shuffle(article_ids)  # and shuffle that list\n",
    "\n",
    "\n",
    "    training_ids = article_ids[:int(training * len(article_ids))]\n",
    "    hold_out_ids = article_ids[int(training * len(article_ids)):]\n",
    "\n",
    "    # write the split body ids out to files for future use\n",
    "    with open(base_dir+ \"/\"+ \"training_ids.txt\", \"w+\") as f:\n",
    "        f.write(\"\\n\".join([str(id) for id in training_ids]))\n",
    "\n",
    "    with open(base_dir+ \"/\"+ \"hold_out_ids.txt\", \"w+\") as f:\n",
    "        f.write(\"\\n\".join([str(id) for id in hold_out_ids]))\n",
    "\n",
    "\n",
    "\n",
    "def read_ids(file,base):\n",
    "    ids = []\n",
    "    with open(base+\"/\"+file,\"r\") as f:\n",
    "        for line in f:\n",
    "           ids.append(int(line))\n",
    "        return ids\n",
    "\n",
    "\n",
    "def kfold_split(dataset, training = 0.8, n_folds = 10, base_dir=\"splits\"):\n",
    "    if not (os.path.exists(base_dir+ \"/\"+ \"training_ids.txt\")\n",
    "            and os.path.exists(base_dir+ \"/\"+ \"hold_out_ids.txt\")):\n",
    "        generate_hold_out_split(dataset,training,base_dir)\n",
    "\n",
    "    training_ids = read_ids(\"training_ids.txt\", base_dir)\n",
    "    hold_out_ids = read_ids(\"hold_out_ids.txt\", base_dir)\n",
    "\n",
    "    folds = []\n",
    "    for k in range(n_folds):\n",
    "        folds.append(training_ids[int(k*len(training_ids)/n_folds):int((k+1)*len(training_ids)/n_folds)])\n",
    "\n",
    "    return folds,hold_out_ids\n",
    "\n",
    "\n",
    "def get_stances_for_folds(dataset,folds,hold_out):\n",
    "    stances_folds = defaultdict(list)\n",
    "    stances_hold_out = []\n",
    "    for stance in dataset.stances:\n",
    "        if stance['Body ID'] in hold_out:\n",
    "            stances_hold_out.append(stance)\n",
    "        else:\n",
    "            fold_id = 0\n",
    "            for fold in folds:\n",
    "                if stance['Body ID'] in fold:\n",
    "                    stances_folds[fold_id].append(stance)\n",
    "                fold_id += 1\n",
    "\n",
    "    return stances_folds,stances_hold_out"
   ],
   "metadata": {
    "id": "Znejl5V2u4yp"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Adapted from https://github.com/FakeNewsChallenge/fnc-1/blob/master/scorer.py\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import json\n",
    "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "LABELS_RELATED = ['unrelated', 'related']\n",
    "RELATED = LABELS[0:3]\n",
    "\n",
    "\n",
    "def score_submission(gold_labels, test_labels):\n",
    "    score = 0.0\n",
    "    cm = [[0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0]]\n",
    "\n",
    "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
    "        g_stance, t_stance = g, t\n",
    "        if g_stance == t_stance:\n",
    "            score += 0.25\n",
    "            if g_stance != 'unrelated':\n",
    "                score += 0.50\n",
    "        if g_stance in RELATED and t_stance in RELATED:\n",
    "            score += 0.25\n",
    "\n",
    "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
    "\n",
    "    return score, cm\n",
    "\n",
    "\n",
    "def print_confusion_matrix(cm):\n",
    "    lines = []\n",
    "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
    "    line_len = len(header)\n",
    "    lines.append(\"-\" * line_len)\n",
    "    lines.append(header)\n",
    "    lines.append(\"-\" * line_len)\n",
    "\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for i, row in enumerate(cm):\n",
    "        hit += row[i]\n",
    "        total += sum(row)\n",
    "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
    "                                                                   *row))\n",
    "        lines.append(\"-\" * line_len)\n",
    "    print('\\n'.join(lines))\n",
    "\n",
    "\n",
    "def report_score(actual, predicted):\n",
    "    score, cm = score_submission(actual, predicted)\n",
    "    best_score, _ = score_submission(actual, actual)\n",
    "\n",
    "    print_confusion_matrix(cm)\n",
    "    print(\"Score: \" + str(score) + \" out of \" + str(best_score) + \"\\t(\" + str(score * 100 / best_score) + \"%)\")\n",
    "    aprf_scores = get_precision_recall_f1_scores(actual, predicted)\n",
    "\n",
    "    print(json.dumps(aprf_scores, indent=4, sort_keys=True, separators=(',', ': ')))\n",
    "    return score * 100 / best_score\n",
    "\n",
    "#Calculating accuracy precision recall f1 scores\n",
    "def get_precision_recall_f1_scores(actual, predicted):\n",
    "    # calculate precision scores for labels - Average None\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true=actual, y_pred=predicted, labels=LABELS, zero_division=0.0,\n",
    "                                                  average=None)\n",
    "\n",
    "    precisions = [{LABELS[index] : value} for (index, value) in enumerate(p)]\n",
    "    recalls = [{LABELS[index] : value} for (index, value) in enumerate(r)]\n",
    "    f1_scores = [{LABELS[index] : value} for (index, value) in enumerate(f1)]\n",
    "    acc = accuracy_score(actual, predicted)\n",
    "    avg_none = {\"accuracy\": acc, \"precision\": precisions, \"recall\": recalls, \"f1\": f1_scores}\n",
    "\n",
    "    # calculate precision scores for labels - Average micro\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true=actual, y_pred=predicted, labels=LABELS, zero_division=0.0,\n",
    "                                                  average='micro')\n",
    "\n",
    "    avg_micro = {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "    # calculate precision scores for labels - Average macro\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true=actual, y_pred=predicted, labels=LABELS, zero_division=0.0,\n",
    "                                                  average='macro')\n",
    "\n",
    "    avg_macro = {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "    # calculate precision scores for labels - Average weighted\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true=actual, y_pred=predicted, labels=LABELS, zero_division=0.0,\n",
    "                                                  average='weighted')\n",
    "\n",
    "    avg_weighted = {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "\n",
    "    return {\"Each_Class\": avg_none, \"micro\": avg_micro, \"macro\": avg_macro, \"weighted\": avg_weighted}"
   ],
   "metadata": {
    "id": "KlZRwGZCqoQ7"
   },
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Adapted from https://github.com/FakeNewsChallenge/fnc-1/blob/master/scorer.py\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m confusion_matrix\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m precision_recall_fscore_support, accuracy_score\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'sklearn'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#adapted from https://github.com/FakeNewsChallenge/fnc-1-baseline/blob/master/feature_engineering.py\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "_wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
    "\n",
    "\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "\n",
    "def gen_or_load_feats(feat_fn, headlines, bodies, feature_file):\n",
    "    if not os.path.isfile(feature_file):\n",
    "        feats = feat_fn(headlines, bodies)\n",
    "        np.save(feature_file, feats)\n",
    "\n",
    "    return np.load(feature_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def word_overlap_features(headlines, bodies):\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        clean_body = get_tokenized_lemmas(clean_body)\n",
    "        features = [\n",
    "            len(set(clean_headline).intersection(clean_body)) / float(len(set(clean_headline).union(clean_body)))]\n",
    "        X.append(features)\n",
    "    return X\n",
    "\n",
    "\n",
    "def refuting_features(headlines, bodies):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        # 'refute',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        features = [1 if word in clean_headline else 0 for word in _refuting_words]\n",
    "        X.append(features)\n",
    "    return X\n",
    "\n",
    "\n",
    "def polarity_features(headlines, bodies):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "\n",
    "    def calculate_polarity(text):\n",
    "        tokens = get_tokenized_lemmas(text)\n",
    "        return sum([t in _refuting_words for t in tokens]) % 2\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        features = []\n",
    "        features.append(calculate_polarity(clean_headline))\n",
    "        features.append(calculate_polarity(clean_body))\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def chargrams(input, n):\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def append_chargrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    grams_first_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "        if gram in text_body[:100]:\n",
    "            grams_first_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    features.append(grams_first_hits)\n",
    "    return features\n",
    "\n",
    "\n",
    "def append_ngrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    return features\n",
    "\n",
    "\n",
    "def hand_features(headlines, bodies):\n",
    "\n",
    "    def binary_co_occurence(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in clean(headline).split(\" \"):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "            if headline_token in clean(body)[:255]:\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def binary_co_occurence_stops(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text. Stopwords in the title\n",
    "        # are ignored.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def count_grams(headline, body):\n",
    "        # Count how many times an n-gram of the title\n",
    "        # appears in the entire body, and intro paragraph\n",
    "\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = clean(headline)\n",
    "        features = []\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 8)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 16)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 3)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 5)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 6)\n",
    "        return features\n",
    "\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        X.append(binary_co_occurence(headline, body)\n",
    "                 + binary_co_occurence_stops(headline, body)\n",
    "                 + count_grams(headline, body))\n",
    "\n",
    "\n",
    "    return X\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QONW7t30zyZV",
    "outputId": "c4988ba0-d88d-4e34-ffa6-d57f73de546f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#adapted from https://github.com/FakeNewsChallenge/fnc-1-baseline/blob/master/fnc_kfold.py\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_features(stances, dataset, name):\n",
    "    h, b, y = [], [], []\n",
    "\n",
    "    for stance in stances:\n",
    "        y.append(LABELS.index(stance['Stance']))\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(dataset.articles[stance['Body ID']])\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, h, b, \"features/overlap.\" + name + \".npy\")\n",
    "    X_refuting = gen_or_load_feats(refuting_features, h, b, \"features/refuting.\" + name + \".npy\")\n",
    "    X_polarity = gen_or_load_feats(polarity_features, h, b, \"features/polarity.\" + name + \".npy\")\n",
    "    X_hand = gen_or_load_feats(hand_features, h, b, \"features/hand.\" + name + \".npy\")\n",
    "\n",
    "    X = np.c_[X_hand, X_polarity, X_refuting, X_overlap]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def generate_features_without_labels(stances, dataset, name):\n",
    "    h, b, y = [], [], []\n",
    "\n",
    "    for stance in stances:\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(dataset.articles[stance['Body ID']])\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, h, b, \"features/overlap.\" + name + \".npy\")\n",
    "    X_refuting = gen_or_load_feats(refuting_features, h, b, \"features/refuting.\" + name + \".npy\")\n",
    "    X_polarity = gen_or_load_feats(polarity_features, h, b, \"features/polarity.\" + name + \".npy\")\n",
    "    X_hand = gen_or_load_feats(hand_features, h, b, \"features/hand.\" + name + \".npy\")\n",
    "\n",
    "    X = np.c_[X_hand, X_polarity, X_refuting, X_overlap]\n",
    "    return X"
   ],
   "metadata": {
    "id": "1xqEisxozsBm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "\n",
    "#check_version()\n",
    "#parse_params()\n",
    "\n",
    "# Load the training dataset and generate folds\n",
    "d = DataSet()\n",
    "folds, hold_out = kfold_split(d, n_folds=10)\n",
    "fold_stances, hold_out_stances = get_stances_for_folds(d, folds, hold_out)\n",
    "\n",
    "# Load the competition dataset\n",
    "unlabeled_competition_dataset = DataSet(name=\"competition_test\", is_unlabeled=True)\n",
    "X_unlabeled = generate_features_without_labels(unlabeled_competition_dataset.stances, unlabeled_competition_dataset,\"competition_unlabeled\")\n",
    "\n",
    "Xs = {}\n",
    "ys = {}\n",
    "\n",
    "# Load/Precompute all features now\n",
    "X_holdout, y_holdout = generate_features(hold_out_stances, d, \"holdout\")\n",
    "for fold in fold_stances:\n",
    "    Xs[fold], ys[fold] = generate_features(fold_stances[fold], d, str(fold))\n",
    "\n",
    "best_score = 0\n",
    "best_fold = None\n",
    "\n",
    "# Classifier for each fold\n",
    "for fold in fold_stances:\n",
    "    ids = list(range(len(folds)))\n",
    "    del ids[fold]\n",
    "\n",
    "    X_train = np.vstack(tuple([Xs[i] for i in ids]))\n",
    "    y_train = np.hstack(tuple([ys[i] for i in ids]))\n",
    "\n",
    "    X_test = Xs[fold]\n",
    "    y_test = ys[fold]\n",
    "\n",
    "    clf = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predicted = [LABELS[int(a)] for a in clf.predict(X_test)]\n",
    "    actual = [LABELS[int(a)] for a in y_test]\n",
    "\n",
    "    fold_score, _ = score_submission(actual, predicted)\n",
    "    max_fold_score, _ = score_submission(actual, actual)\n",
    "\n",
    "    score = fold_score / max_fold_score\n",
    "\n",
    "    print(\"Score for fold \" + str(fold) + \" was - \" + str(score))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_fold = clf\n",
    "\n",
    "# Run on Holdout set and report the final score on the holdout set\n",
    "predicted = [LABELS[int(a)] for a in best_fold.predict(X_holdout)]\n",
    "actual = [LABELS[int(a)] for a in y_holdout]\n",
    "\n",
    "print(\"Scores on the dev set\")\n",
    "report_score(actual, predicted)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# Run on competition dataset\n",
    "predicted = [LABELS[int(a)] for a in best_fold.predict(X_unlabeled)]\n",
    "print(predicted)\n",
    "\n",
    "result = []\n",
    "for i, predictedLabel in enumerate(predicted):\n",
    "    dict = {\n",
    "        \"Headline\": unlabeled_competition_dataset.stances[i]['Headline'],\n",
    "        \"Body ID\": unlabeled_competition_dataset.stances[i]['Body ID'],\n",
    "        \"Stance\": predictedLabel\n",
    "    }\n",
    "    result.append(dict)\n",
    "\n",
    "test_data = pd.DataFrame(result)\n",
    "test_data.to_csv('answer.csv', index=False, encoding='utf-8')  # From pandas library\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OI9pyR7F0KmG",
    "outputId": "529760a7-2816-4726-f646-62873e0d52c5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Received 8748.75 points on leatherboard. Default approach for benchmarking."
   ],
   "metadata": {
    "id": "rydfrPc375Vj"
   }
  }
 ]
}